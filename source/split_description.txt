The split function uses the annotation of the different alignment tools aswell as input and infrastructure properties to decide whether the sequencing input should be split into several chunks. In order to do so, it predicts the runtime of the alignment task assuming different degrees of parallelism.
We use the median size of the read inputs aswell as the CPU and RAM of the executing infrastructure to predict those runtimes. The CPU and RAM are normalized in relation to the RAM and CPU values of the clusters in the annotation database. In the following, the "nearest neighbor" cluster in the annotation is found by minimizing the euclidean distance between the infrastructure the experiments are run on and the infrastructures in the annotation database. 
Subsequently, from the "nearest neighbor" cluster, the estimation model of the alignment task used in the workflow is used to predict the task runtime of the alignment task. This linear estimation model only takes the median input size into account. Using a polynomial model with higher degrees led to overfitting while testing and provided unreasonable results for unseen datapoints. Similarly, using more input features such as CPU, RAM or reference genome size resulted in overfitting to single datapoints - especially since the annotation database (so far) only contains reference datapoints with few different values for CPU, RAM and reference genome size.
The split function first calculates the estimated runtime using this estimation model for running on one node. It then subsequently increases the degree of parallelism, assuming 2, 3, ... chunks and calculates both the savings in runtime by distributing the alignment task and the additional overhead caused by the split and merge tasks. Whenever a degree of parallelism greater than 1 reduces the estimated overall runtime and there is an equal or higher number of nodes available, the read file is split into as many chunks as there are nodes.
